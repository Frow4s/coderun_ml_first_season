{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Здесь нам предлагают решить известную задачу про Многорукого бандита и намекают на использование Томпсоновского сэмплирования тем, что пишут, что вероятности выигрыша у каждого автомата выбираются из бета-распределения.\n",
    "Собственно, ниже представлена реализация этого метода решения задачи.\n",
    "Env - среда, в которой происходит взаимодействие с автоматами, в которой мы выбираем автомат, получаем награду и обновляем состояние нашего агента.\n",
    "BaseSampler - базовый класс агента, ThompsonSampler - агент на основе Томпсоновского сэмплирования.\n",
    "Более подробно можно почитать [тут](https://habr.com/ru/articles/689364/). Еще более подробно в книгах, которые указал автор статьи, я лично дальше общих познаний пока не продвинулся."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, variants, n_trials):\n",
    "        self.variants = variants\n",
    "        self.n_trials = n_trials\n",
    "        self.total_reward = 0\n",
    "        self.n_k = len(variants)\n",
    "        self.shape = (self.n_k, n_trials)\n",
    "\n",
    "    def run(self, agent):\n",
    "        for i in range(self.n_trials):\n",
    "            # делаем выбор\n",
    "            x_chosen = agent.choose_k()\n",
    "            # получаем награду\n",
    "            reward = int(input())\n",
    "            # запоминаем награду\n",
    "            agent.reward = reward\n",
    "            # агент обновляет параметры\n",
    "            agent.update()\n",
    "            self.total_reward += reward\n",
    "\n",
    "        return self.total_reward\n",
    "\n",
    "\n",
    "class BaseSampler:\n",
    "    def __init__(self, env, n_samples=None, n_learning=None, e=0.05, alpha=1, beta=1):\n",
    "        self.env = env\n",
    "        self.shape = (env.n_k, n_samples)\n",
    "        self.variants = env.variants\n",
    "        self.n_trials = env.n_trials\n",
    "        self.ad_i = np.zeros(env.n_trials)\n",
    "        self.r_i = np.zeros(env.n_trials)\n",
    "        self.thetas = np.zeros(self.n_trials)\n",
    "        self.regret_i = np.zeros(env.n_trials)\n",
    "        self.thetaregret = np.zeros(self.n_trials)\n",
    "\n",
    "        self.a = np.full(env.n_k, alpha)\n",
    "        self.b = np.full(env.n_k, beta)\n",
    "        self.theta = np.zeros(env.n_k)\n",
    "        self.data = None\n",
    "        self.reward = 0\n",
    "        self.total_reward = 0\n",
    "        self.k = 0\n",
    "        self.i = 0\n",
    "\n",
    "        self.n_samples = n_samples\n",
    "        self.n_learning = n_learning\n",
    "        self.e = e\n",
    "        self.ep = np.random.uniform(0, 1, size=env.n_trials)\n",
    "        self.exploit = (1 - e)\n",
    "\n",
    "\n",
    "class ThompsonSampler(BaseSampler):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def choose_k(self):\n",
    "        self.theta = np.random.beta(self.a, self.b)\n",
    "        # выбираем автомат\n",
    "        self.k = self.variants[np.argmax(self.theta)]\n",
    "        # выводим номер автомата\n",
    "        print(self.k+1)\n",
    "\n",
    "        return self.k\n",
    "\n",
    "    def update(self):\n",
    "        self.a[self.k] += self.reward\n",
    "        self.b[self.k] += 1 - self.reward\n",
    "\n",
    "        self.thetas[self.i] = self.theta[self.k]\n",
    "        self.thetaregret[self.i] = np.max(self.thetas) - self.theta[self.k]\n",
    "\n",
    "        self.ad_i[self.i] = self.k\n",
    "        self.r_i[self.i] = self.reward\n",
    "        self.i += 1\n",
    "\n",
    "\n",
    "while True:\n",
    "    n, m = map(int, input().split())\n",
    "    if n == 0 and m == 0:\n",
    "        break\n",
    "    a, b = map(float, input().split())\n",
    "    machines = [i for i in range(m)]\n",
    "    en = Environment(machines, n_trials=n)\n",
    "    sampler = ThompsonSampler(env=en)\n",
    "    en.run(agent=sampler)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
