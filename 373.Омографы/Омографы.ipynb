{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "В данной задаче была использована библиотека LightAutoML. AutoML говорит о том, что большая часть была сделана автоматически с помощью встроенного класса TabularNLPAutoML. С моей стороны задача была поставлена так: на вход подается слово и текст, на выходе необходимо получать номер гласной в слове, на которую падает ударение. В качестве NLP-модели для fine-tuning была взята rubert-base-cased-conversational от DeepPavlov. В качестве типа задачи была выбрана классификация."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                              source\n0    до АВГУСТА , я думаю , должно что проясниться .\n1      заседание перенесли на шестнадцатое АВГУСТА .\n2   АВГУСТА - женское имя латинского происхождения .\n3  бабушка невилла АВГУСТА поддерживала переписку...\n4                                             АДОНИС",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>до АВГУСТА , я думаю , должно что проясниться .</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>заседание перенесли на шестнадцатое АВГУСТА .</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>АВГУСТА - женское имя латинского происхождения .</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>бабушка невилла АВГУСТА поддерживала переписку...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>АДОНИС</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train =  pd.read_json('train.json')\n",
    "test = pd.read_json('test.json')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "vowels = set('аоуыэеёиюя')\n",
    "def get_stress_num(row):\n",
    "    text = row['target'].split()\n",
    "    cntr = 0\n",
    "    for word in text:\n",
    "        if sum(map(str.isupper,word)) == 1:\n",
    "            for c in word:\n",
    "                if c.isupper():\n",
    "                    return cntr\n",
    "                if c.lower() in vowels:\n",
    "                    cntr += 1\n",
    "\n",
    "def get_word(row):\n",
    "    text = row['source']\n",
    "    text = text.split()\n",
    "    for word in text:\n",
    "        if word.isupper():\n",
    "            return word\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train['word'] = train.apply(get_word,axis=1)\n",
    "train['vowel_num'] = train.apply(get_stress_num,axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train = train.drop('target',axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "0    60932\n1    57992\n2    18707\n3     2938\n4      298\n5       87\nName: vowel_num, dtype: int64"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.vowel_num.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from lightautoml.automl.presets.text_presets import TabularNLPAutoML\n",
    "from lightautoml.tasks import Task\n",
    "\n",
    "roles = {\n",
    "    'text': ['source','word'],\n",
    "    'target': 'vowel_num'\n",
    "}\n",
    "\n",
    "task = Task('multiclass',metric='crossentropy')\n",
    "\n",
    "automl = TabularNLPAutoML(\n",
    "    task=task,\n",
    "    timeout=30000,\n",
    "    cpu_limit=8,\n",
    "    general_params={\n",
    "        'nested_cv': False,\n",
    "        'use_algos': [['nn']],\n",
    "    },\n",
    "    nested_cv_params = {\n",
    "        'cv': 3\n",
    "    },\n",
    "    text_params={\n",
    "        'lang': 'ru',\n",
    "        'bert_model': 'DeepPavlov/rubert-base-cased-conversational'\n",
    "    },\n",
    "    nn_params={\n",
    "        'opt_params': {'lr': 1e-5},\n",
    "        'max_length': 150,\n",
    "        'bs': 32,\n",
    "        'n_epochs': 4,\n",
    "    },\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased-conversational were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "train (loss=0.703459): 100%|██████████| 2937/2937 [15:07<00:00,  3.23it/s]\n",
      "val: 100%|██████████| 1469/1469 [04:26<00:00,  5.51it/s]\n",
      "train (loss=0.321353): 100%|██████████| 2937/2937 [14:53<00:00,  3.29it/s]\n",
      "val: 100%|██████████| 1469/1469 [04:26<00:00,  5.51it/s]\n",
      "train (loss=0.183352): 100%|██████████| 2937/2937 [14:50<00:00,  3.30it/s]\n",
      "val: 100%|██████████| 1469/1469 [04:30<00:00,  5.43it/s]\n",
      "train (loss=0.126085): 100%|██████████| 2937/2937 [14:50<00:00,  3.30it/s]\n",
      "val: 100%|██████████| 1469/1469 [04:26<00:00,  5.52it/s]\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased-conversational were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "train (loss=0.728785): 100%|██████████| 2937/2937 [14:49<00:00,  3.30it/s]\n",
      "val: 100%|██████████| 1469/1469 [04:29<00:00,  5.45it/s]\n",
      "train (loss=0.322142): 100%|██████████| 2937/2937 [17:19<00:00,  2.83it/s]\n",
      "val: 100%|██████████| 1469/1469 [04:56<00:00,  4.96it/s]\n",
      "train (loss=0.186867): 100%|██████████| 2937/2937 [15:30<00:00,  3.16it/s]\n",
      "val: 100%|██████████| 1469/1469 [04:35<00:00,  5.34it/s]\n",
      "train (loss=0.129547): 100%|██████████| 2937/2937 [16:10<00:00,  3.03it/s]\n",
      "val: 100%|██████████| 1469/1469 [04:26<00:00,  5.52it/s]\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased-conversational were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "train (loss=0.778425): 100%|██████████| 2937/2937 [14:38<00:00,  3.34it/s]\n",
      "val: 100%|██████████| 1469/1469 [04:43<00:00,  5.18it/s]\n",
      "train (loss=0.353069): 100%|██████████| 2937/2937 [15:39<00:00,  3.13it/s]\n",
      "val: 100%|██████████| 1469/1469 [04:54<00:00,  4.98it/s]\n",
      "train (loss=0.193831): 100%|██████████| 2937/2937 [16:01<00:00,  3.05it/s]\n",
      "val: 100%|██████████| 1469/1469 [05:15<00:00,  4.65it/s]\n",
      "train (loss=0.130507): 100%|██████████| 2937/2937 [15:42<00:00,  3.12it/s]\n",
      "val: 100%|██████████| 1469/1469 [04:46<00:00,  5.13it/s]\n"
     ]
    }
   ],
   "source": [
    "oof_pred = automl.fit_predict(train, roles=roles)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "test['word'] = test.apply(get_word,axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased-conversational were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "test: 100%|██████████| 534/534 [01:42<00:00,  5.20it/s]\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased-conversational were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "test: 100%|██████████| 534/534 [01:42<00:00,  5.22it/s]\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased-conversational were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "test: 100%|██████████| 534/534 [01:41<00:00,  5.24it/s]\n"
     ]
    }
   ],
   "source": [
    "res = automl.predict(test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 1, 2, 3], dtype=int64)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(res.to_numpy().data.argmax(axis=1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "test['pred'] = res.to_numpy().data.argmax(axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def prediction(row):\n",
    "    word = row['word']\n",
    "    vowel_num = row['pred']\n",
    "    cntr = 0\n",
    "    for indx,c in enumerate(word):\n",
    "        if c.lower() in vowels:\n",
    "            if cntr == vowel_num:\n",
    "                new_word = word.lower()\n",
    "                new_word = new_word[:indx] + new_word[indx].upper() + new_word[indx+1:]\n",
    "                return new_word\n",
    "            cntr += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "test['predicted_word']  = test.apply(prediction,axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "test['predicted_word'].to_csv('word.csv',index=None,header=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
